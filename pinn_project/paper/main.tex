\documentclass{iopjournal}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{multirow}

\graphicspath{{figures/}}

\begin{document}

\articletype{Paper}

\title{Multi-Method Validation Framework for Teaching Radiation Transport: Comparing Monte Carlo, Analytical, and Physics-Informed Machine Learning Approaches}

\author{Yash Varshney$^{1,*}$, Abhimanyu Agarwal$^{1}$}

\affil{$^1$Gurukul The School, Ghaziabad, Uttar Pradesh, India}

\affil{$^*$Author to whom any correspondence should be addressed.}

\keywords{physics education, radiation transport, Monte Carlo simulation, physics-informed neural networks, scintillator detectors, validation methodology, proton therapy education}

\begin{abstract}
Students in computational physics courses increasingly rely on sophisticated simulation tools and machine learning models, yet often treat these as ``black boxes'' without developing the critical skill of multi-method validation. This pedagogical gap is concerning: in professional physics practice, validating computational predictions against independent methods is essential for ensuring reliability. We present an educational framework that teaches multi-method validation through radiation transport by systematically comparing Monte Carlo simulation (OpenTOPAS), analytical theory (Bethe-Bloch), and physics-informed neural netwphysics-informed neural networks. Using proton energy deposition in plastic scintillator detectors as an accessible case study, students learn to interpret when models agree, when they disagree, and, crucially, why discrepancies often reveal physics rather than computational error. The framework demonstrates that embedding physics constraints (specifically smoothness) into machine learning improves interpolation to unseen conditions by approximately 13%, providing a concrete example of how domain knowledge enhances AI predictions. Systematic ablation studies serve as a teaching exercise showing that not all physics constraints help equally; indeed, some can conflict with simulation data, teaching students to validate their constraints, not just their models. Monte Carlo simulations validated against NIST reference data teach students to interpret systematic deviations as signatures of additional physics (nuclear interactions) rather than errors. All simulation configurations, pre-trained models, Jupyter notebooks, and instructor guides are provided as open educational resources, enabling direct classroom adoption. This framework prepares students for careers where validating computational tools, whether in research, industry, or medical physics, is a core professional responsibility.
\end{abstract}

%% ============================================================================
%% SECTION 1: INTRODUCTION
%% ============================================================================
\section{Introduction}

\subsection{Why Validation Matters in Physics Education}

The ability to validate computational results is among the most important, yet most undertaught, skills in modern physics education. As the American Association of Physics Teachers has emphasized, computational literacy must include not just the ability to write simulations, but the critical judgment to assess when those simulations can be trusted \cite{AAPT2016}. Yet undergraduate physics curricula rarely provide structured opportunities for students to practice multi-method validation: the systematic comparison of results from fundamentally different calculation approaches.

This gap has consequences. Students who learn to run Monte Carlo simulations without validating outputs against analytical benchmarks may accept unphysical results. Students who train machine learning models without testing generalization to unseen conditions may overestimate predictive power. Most critically, students who never see methods disagree miss the profound learning that emerges from understanding \textit{why} discrepancies arise.

\subsection{Why Modern Tools Make Validation Harder}

Two developments in computational physics have made the validation challenge more acute:

\textbf{Monte Carlo simulation complexity.} Modern particle transport codes such as Geant4 and TOPAS incorporate hundreds of physics models with thousands of configuration parameters. Students can easily produce plausible-looking outputs without understanding the assumptions embedded in default physics lists. The ``black box'' concern is not hypothetical; rather, it reflects how students actually interact with these tools.

\textbf{Machine learning opacity.} Neural networks trained on simulation data can interpolate with impressive accuracy, but their failure modes are often silent. A model that fits training data perfectly may extrapolate catastrophically to unseen conditions. Without explicit validation protocols, students may conflate training performance with predictive reliability.

\subsection{What Students Typically Misunderstand}

Our experience teaching computational physics reveals several persistent misconceptions that multi-method validation directly addresses:

\begin{itemize}
    \item \textbf{``Monte Carlo is ground truth.''} Students often view simulation outputs as inherently correct, not recognizing that simulations encode physics approximations with limited domains of validity.
    \item \textbf{``Good training loss means good predictions.''} Machine learning courses emphasize minimizing loss functions, but students may not appreciate that generalization to unseen data requires validation beyond the training set.
    \item \textbf{``Disagreement means error.''} When methods disagree, students assume one must be wrong. The educational insight is that disagreement often signals interesting physics---different methods may answer different physical questions.
    \item \textbf{``Analytical formulas are always applicable.''} Textbook formulas like Bethe-Bloch are presented without emphasis on their domains of validity, leading students to apply approximations beyond their intended regimes.
\end{itemize}

\subsection{What This Framework Uniquely Teaches}

We present an educational framework that teaches validation through a ``Validation Triad'' applied to radiation transport:

\begin{itemize}
    \item \textbf{Theoretical Benchmarks:} Analytical models (Bethe-Bloch formula) providing physical insight and asymptotic limits, with explicit attention to where approximations break down.
    \item \textbf{Stochastic Simulation:} Monte Carlo codes (OpenTOPAS/Geant4) capturing complex particle interactions and geometry effects that analytical theory cannot easily represent.
    \item \textbf{Physics-Informed Machine Learning:} Neural networks that bridge theory and simulation, demonstrating concretely how domain knowledge improves AI predictions.
\end{itemize}

Using proton energy deposition in EJ-200 plastic scintillators as an accessible case study, students observe how these methods reveal complementary aspects of the underlying physics. Discrepancies between analytical theory and Monte Carlo simulation at high energies serve as a teaching moment for identifying ``missing physics'' (nuclear interactions), while the physics-informed neural network demonstrates how embedding physical constraints improves generalization to conditions not seen during training.

\subsection{Educational Contributions}

This framework provides a complete, open-source validation laboratory designed for classroom adoption:

\begin{enumerate}
    \item \textbf{Multi-method comparison:} A worked example comparing Monte Carlo, analytical, and physics-informed ML approaches, with explicit discussion of where each method succeeds and fails.
    \item \textbf{Quantitative demonstration of physics constraints:} Ablation studies showing that embedding domain knowledge in neural networks substantially improves generalization, and that constraint selection requires validation.
    \item \textbf{Reference-validated simulation dataset:} 140,000 proton histories across 14 energies, validated against NIST PSTAR reference data, with systematic analysis of discrepancies.
    \item \textbf{Extensible codebase:} Students can modify detector materials, energy ranges, and physics constraints to explore validation independently.
\end{enumerate}

\textbf{Target audience:} This framework is designed for advanced undergraduate (3rd--4th year) or early MSc students in computational physics courses, nuclear instrumentation laboratories, or introductory medical physics modules. Prerequisites include familiarity with Python programming and basic particle physics concepts.

\textbf{Scope:} While utilizing tools from medical physics (TOPAS), this work focuses on detector physics principles and validation methodology. It is designed as an educational resource, not as a guide for clinical dose calculation.

%% ============================================================================
%% SECTION 2: THEORETICAL BACKGROUND
%% ============================================================================
\section{Theoretical Background}

\subsection{Proton Energy Loss in Matter}

% EQUATION REMOVED: Bethe-Bloch formula
The mean energy loss per unit path length is described by the well-known Bethe-Bloch formula \cite{Bethe1930, Bloch1933}. For the purposes of this educational framework, the key physical insight is that stopping power scales approximately as $\beta^{-2}$ (where $\beta = v/c$) at lower energies. As the particle slows down, $\beta$ decreases, and energy deposition rises dramatically---creating the Bragg peak. At relativistic energies ($E > 200$~MeV), the curve flattens and then rises slightly due to the relativistic expansion of the electric field (density effect), though this rise is often masked by nuclear interactions in thick detectors.

At relativistic energies ($E > 200$~MeV), nuclear interactions become significant, producing hadronic cascades that enhance energy deposition beyond electronic stopping power predictions \cite{ICRU2014}. This physics distinction between electronic-only (NIST) and full-simulation (TOPAS) stopping powers provides educational value in analyzing apparent discrepancies.

\subsection{EJ-200 Plastic Scintillator}

We selected EJ-200 plastic scintillator (Eljen Technology) as our detector material due to its well-characterized properties and relevance to particle physics instrumentation. Table~\ref{tab:material} summarizes the verified material properties.

\begin{table}[htb]
\caption{EJ-200 plastic scintillator properties from manufacturer datasheet and NIST. All values independently verified.}
\centering
\begin{tabular}{l l l}
\toprule
\textbf{Property} & \textbf{Value} & \textbf{Source} \\
\midrule
Density & 1.023~g/cm$^3$ & Eljen datasheet \\
Base polymer & Polyvinyltoluene (PVT) & Eljen datasheet \\
Composition (H:C) & C$_9$H$_{10}$ (1.10:1) & NIST \\
H weight fraction & 0.0847 & Calculated \\
C weight fraction & 0.9153 & Calculated \\
Mean excitation energy & 64.7~eV & NIST \\
Light yield & 10,000 photons/MeV & Eljen datasheet \\
Decay time & 2.1~ns & Eljen datasheet \\
Birks' constant & 0.1232~mm/MeV & Literature \\
\bottomrule
\end{tabular}
\label{tab:material}
\end{table}

\subsection{Physics-Informed Neural Networks}

% EQUATION REMOVED: PINN loss function
In a standard neural network, the training process adjusts the model parameters to reduce the discrepancy between predictions and training data (mean squared error). A Physics-Informed Neural Network (PINN) fundamentally alters this objective by adding a ``physics regularization'' term. The model training thus becomes a balancing act: finding a solution that fits the observed data while simultaneously satisfying known physical laws (smoothness, positivity, boundary conditions) \cite{Karniadakis2021}. This forces the network to learn solution spaces that are physically plausible, rather than just mathematically possible.

%% ============================================================================
%% SECTION 3: METHODS
%% ============================================================================
\section{Methods: A Validation Laboratory}

\textit{Pedagogical Note:} This section is written to guide students through the decision-making process of setting up a validation study. Rather than simply listing parameters, we emphasize the physical justifications for each computational choice.

\subsection{Step 1: Establishing Ground Truth with Monte Carlo}
Our first validation anchor is a stochastic simulation. We utilized the OpenTOPAS toolkit \cite{Perl2012} to model the passage of protons through the scintillator. The critical educational step here is not just running the code, but selecting the appropriate ``physics list'', representing the set of theoretical models the code uses.

We selected the \texttt{g4em-standard\_opt4} configuration because standard electromagnetic settings often lack the precision required for low-energy proton stopping. For nuclear interactions, we enabled the \texttt{QGSP\_BIC\_HP} physics list, which models the complex hadronic cascades that occur above 200~MeV. This choice is deliberate: by including nuclear physics in our ``ground truth'' simulation, we ensure it captures phenomena that our analytical benchmarks (which ignore nuclear effects) will miss. This sets the stage for a meaningful disagreement between methods later in the study.

\textbf{Simulation Setup:} To mirror a realistic experiment, we simulated a 1\% energy spread and a 10,000-history statistical budget per energy. This introduces a realistic level of statistical noise ($\sim$1\%), preventing the neural network from effortlessly fitting a perfect curve and forcing it to separate signal from noise.

\subsection{Step 2: Defining Analytical Benchmarks}
Before training any AI models, students must establish what ``theory'' predicts. We calculate the expected stopping power using the NIST PSTAR database values for polystyrene. Crucially, students are instructed that this reference represents \textit{electronic stopping power only}.

\begin{itemize}
    \item \textbf{When they agree:} At low energies (<200 MeV), NIST and TOPAS should match closely.
    \item \textbf{When they diverge:} At high energies (>1 GeV), we \textit{expect} TOPAS to predict higher energy deposition due to nuclear interactions.
\end{itemize}

This pre-analysis framing transforms the validation from a generic ``accuracy check'' into a physics experiment: we are testing where the electronic stopping approximation holds.

\subsection{Step 3: Designing the Neural Network}
We treat the neural network not as a magic box, but as a flexible function approximator that needs guidance. We utilized a standard feedforward architecture (4 hidden layers, 64 neurons each) rather than a complex deep learning model. The educational message is that \textit{model architecture matters less than physical constraints}. A simple network constrained by physics will outperform a complex network that ignores it.

\textbf{Inputs and Outputs:} The model predicts stopping power ($dE/dx$) based on proton energy and depth. We specifically included physics-derived features (velocity $\beta$, Lorentz factor $\gamma$) to give the network a "head start" in learning the relevant non-linearities.

\subsection{Step 4: Embedding Physics Constraints}
The core teaching activity involves "teaching physics" to the neural network. We do this by modifying how the network measures error. Instead of asking ``Did I match the data?'', the PINN asks ``Did I match the data AND obey the laws of physics?''

We implemented three distinct constraints to test which physical principles are most valuable:

\begin{enumerate}
    \item \textbf{Smoothness (The Continuity Principle):} We penalize "rough" predictions (large second derivatives). This teaches the network that real energy deposition profiles are smooth continuous functions, preventing it from overfitting to statistical noise.
    
    \item \textbf{Positivity (The Realism Principle):} We heavily penalize any prediction of negative energy deposition. This is a simple but powerful sanity check that standard networks often fail without explicit instruction.
    
    \item \textbf{Gradient Correlation (The Theoretical Principle):} We attempt to force the network's shape to match the Bethe-Bloch theory gradient. As we will see in the Results, this constraint captures the subtle danger of enforcing theory that may be only approximately true.
\end{enumerate}

\subsection{Step 5: Validating the Constraints}
\textit{Student Exercise:} Which constraint helps most?
We performed a systematic "ablation study," turning constraints on and off to measure their impact. This teaches students that adding physics constraints is not automatically beneficial; each constraint represents a bias that must be validated resulting in the selection of the most effective configuration ($\lambda_s=0.1$, $\lambda_p=0.05$, $\lambda_g=0$).



%% ============================================================================
%% SECTION 4: RESULTS
%% ============================================================================
\section{Results}

\subsection{Monte Carlo Dose-Depth Profiles}

Figure~\ref{fig:dose_profiles} presents the energy deposition profiles across all 14 simulated energies, demonstrating the characteristic physics of proton stopping.

\begin{figure}[htb]
\centering
\includegraphics[width=0.9\textwidth]{dose_profiles}
\caption{\textit{What students should observe:} Energy deposition profiles for protons in EJ-200 scintillator across energies from 70~MeV to 6~GeV. At lower energies (70--250~MeV), the characteristic Bragg peak is visible within the 10~mm detector. At higher energies, protons traverse the detector with approximately constant energy loss. This figure teaches students that detector thickness relative to particle range determines whether the Bragg peak is observable. Each curve represents 10,000 simulated proton histories.}
\label{fig:dose_profiles}
\end{figure}

At lower proton energies (70--250~MeV), the characteristic Bragg peak appears within the detector volume (Figure~\ref{fig:bragg_peaks}). At relativistic energies ($>$1~GeV), protons traverse the thin detector with approximately constant energy loss on the $\beta^{-2}$ portion of the Bethe-Bloch curve before rising again due to density effects.

\begin{figure}[htb]
\centering
\includegraphics[width=0.8\textwidth]{bragg_peaks}
\caption{\textit{What students should observe:} Bragg peaks for lower-energy protons (70--300~MeV) showing peaked energy deposition at end-of-range. Students learn the physical principle: higher incident energy results in deeper peak position. This relationship---energy determining range---is fundamental to understanding how proton range can be controlled. At 70~MeV, the Bragg peak occurs at approximately 4~mm depth in EJ-200.}
\label{fig:bragg_peaks}
\end{figure}

\subsection{NIST Reference Validation}

Table~\ref{tab:nist} and Figure~\ref{fig:nist} compare TOPAS-simulated stopping powers against NIST PSTAR reference values.

\begin{table}[htb]
\caption{\textit{Teaching point:} Comparison of simulated stopping power against NIST PSTAR reference data. Students should notice that ratios greater than 1.0 systematically increase with energy. This trend reveals nuclear interaction contributions included in TOPAS but absent from NIST's electronic-only tables. The discrepancy is physics, not error.}
\centering
\begin{tabular}{c c c c l}
\toprule
\textbf{Energy} & \textbf{TOPAS $dE/dx$} & \textbf{NIST $dE/dx$} & \textbf{Ratio} & \textbf{Assessment} \\
\textbf{(MeV)} & \textbf{(MeV/mm)} & \textbf{(MeV/mm)} & & \\
\midrule
70 & 1.049 & 0.958 & 1.09 & \checkmark~Good \\
100 & 0.788 & 0.730 & 1.08 & \checkmark~Good \\
150 & 0.590 & 0.590 & 1.00 & \checkmark~Excellent \\
200 & 0.499 & 0.450 & 1.11 & \checkmark~Good \\
500 & 0.316 & 0.274 & 1.15 & \checkmark~Expected \\
1000 & 0.280 & 0.220 & 1.27 & Nuclear effects \\
2000 & 0.245 & 0.201 & 1.22 & Nuclear effects \\
6000 & 0.310 & 0.202 & 1.54 & Strong nuclear \\
\midrule
\multicolumn{3}{l}{\textbf{Average (70--500 MeV):}} & \textbf{1.09 $\pm$ 0.06} & \\
\multicolumn{3}{l}{\textbf{Average (all energies):}} & \textbf{1.17 $\pm$ 0.17} & \\
\bottomrule
\end{tabular}
\label{tab:nist}
\end{table}

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{nist_comparison}
\caption{\textit{What students should observe:} Simulated stopping power (TOPAS) versus NIST PSTAR reference as a function of proton energy. Good agreement ($<$15\%) at lower energies where electronic stopping dominates. Increasing deviation at higher energies reveals nuclear interaction contributions. This teaches students that apparent ``disagreement'' between methods often reflects complementary physics rather than computational error.}
\label{fig:nist}
\end{figure}

At lower energies (70--250~MeV), agreement is within 11\%, validating the simulation against established reference data. The systematic excess at higher energies illustrates an important teaching point: before assuming computational error, students should check whether the comparison is physically valid. NIST PSTAR reports only electronic stopping power, while TOPAS includes nuclear interactions that contribute 20--50\% additional energy deposition above 1~GeV \cite{ICRU2014}.

\subsection{Feature Correlations}

Figure~\ref{fig:correlation} displays the correlation structure among input features and target, informing model design and identifying potential redundancies.

\begin{figure}[htb]
\centering
\includegraphics[width=0.6\textwidth]{correlation}
\caption{\textit{What students should observe:} Correlation heatmap among input features and stopping power target. Students learn that physics-derived features ($\beta$, $\gamma$, $p$) are highly correlated---a lesson in feature engineering informed by domain knowledge. The anticorrelation between energy and stopping power reflects the Bethe-Bloch $\beta^{-2}$ dependence: faster particles lose less energy per unit length.}
\label{fig:correlation}
\end{figure}

\subsection{Model Performance Comparison}

Table~\ref{tab:results} summarizes model performance on the withheld test energies (200 and 750~MeV).

\begin{table}[htb]
\caption{\textit{Teaching point:} Test set performance comparing baseline and physics-informed neural networks on unseen energies. Students observe that embedding physics constraints substantially improves interpolation to conditions not seen during training, thereby demonstrating concretely how domain knowledge enhances AI predictions.}
\centering
\begin{tabular}{l c c c c c}
\toprule
\textbf{Model} & \textbf{MSE} & \textbf{MAE} & \textbf{MAPE (\%)} & \textbf{Pred. Uncert. (\%)} & \textbf{Improvement} \\
\midrule
Baseline NN & $5.70 \times 10^{-4}$ & $1.93 \times 10^{-2}$ & 17.13 & 5.8 & --- \\
\textbf{Physics-Informed NN} & $\mathbf{4.94 \times 10^{-4}}$ & $\mathbf{1.75 \times 10^{-2}}$ & \textbf{12.01} & \textbf{3.2} & \textbf{+13.3\%} \\
\bottomrule
\end{tabular}
\label{tab:results}
\end{table}

The physics-informed model (utilizing the smoothness constraint) reduced prediction error on unseen test energies. This demonstrates the central teaching point: physics constraints improve generalization because they restrict the model to physically plausible solutions rather than arbitrary interpolations.

\subsection{Ablation Study: A Teaching Demonstration}

This section provides a particularly valuable teaching exercise: systematically investigating which physics constraints actually help. Students often assume that ``more physics is better,'' but the ablation study reveals a more nuanced reality (Table~\ref{tab:ablation}).

\begin{table}[htb]
\caption{\textit{Teaching demonstration:} Ablation study showing individual and combined physics constraint contributions. Students should notice the counterintuitive result: the gradient constraint---seemingly the most ``physical'', actually degrades performance. This teaches that physics constraints must themselves be validated.}
\centering
\begin{tabular}{l c c c c c}
\toprule
\textbf{Configuration} & \textbf{$\lambda_s$} & \textbf{$\lambda_p$} & \textbf{$\lambda_g$} & \textbf{Test MSE} & \textbf{Improvement} \\
\midrule
Baseline (no constraints) & 0 & 0 & 0 & $5.70 \times 10^{-4}$ & --- \\
\textbf{Smoothness only} & 0.1 & 0 & 0 & $\mathbf{4.94 \times 10^{-4}}$ & \textbf{+13.3\%} \\
Positivity only & 0 & 0.01 & 0 & $8.64 \times 10^{-4}$ & $-51.4\%$ \\
Gradient only & 0 & 0 & 0.05 & $1.10 \times 10^{-3}$ & $-92.5\%$ \\
Smooth + Pos & 0.1 & 0.01 & 0 & $5.96 \times 10^{-4}$ & $-4.5\%$ \\
Smooth + Grad & 0.1 & 0 & 0.05 & $9.79 \times 10^{-4}$ & $-71.6\%$ \\
Full PINN (All constraints) & 0.1 & 0.01 & 0.05 & $5.31 \times 10^{-4}$ & +6.9\% \\
\bottomrule
\end{tabular}
\label{tab:ablation}
\end{table}

\textit{Key lessons for students from the ablation study:}

\begin{enumerate}
    \item \textbf{Smoothness has strong physical justification:} The smoothness constraint alone achieves the best performance (+13.3\% optimization) because dose distributions must vary continuously in space. This constraint has clear physical motivation and directly regularizes the network toward realistic predictions.
    
    \item \textbf{``Physical'' constraints can conflict with data:} The gradient correlation constraint---based on the Bethe-Bloch formula, substantially degrades performance (-92.5\%). This teaches students a crucial lesson: idealized formulas have domains of validity, and forcing the network to match them can conflict with the realistic physics (like nuclear interactions) captured in the data.
    
    \item \textbf{Constraints can compete:} Adding positivity to smoothness actually reduced performance (-4.5\%), contrasting with smoothness alone. This highlights that physics-informed ML requires thoughtful constraint selection through validation, not just adding every plausible constraint.
\end{enumerate}

These results motivate our constraint weight selection and illustrate why physics-informed machine learning requires understanding both the physics and its limitations.

\subsection{Training Dynamics}

Figure~\ref{fig:training} illustrates the training dynamics for both models.

\begin{figure}[htb]
\centering
\includegraphics[width=0.8\textwidth]{training_history}
\caption{\textit{What students should observe:} Training dynamics of the physics-informed neural network over 98 epochs. Students can track how each physics constraint converges during training. The smoothness constraint converges to very small values ($\sim$10$^{-6}$), indicating the model learned physically realistic smooth predictions. Early stopping prevented overfitting---visible as the point where validation loss begins to increase.}
\label{fig:training}
\end{figure}

The baseline model converged rapidly (32 epochs) but with higher test error. The PINN required 98 epochs for the physics constraints to fully shape the solution space, ultimately achieving better generalization. Notably, the smoothness loss decreased from $1.79 \times 10^{-3}$ to $1 \times 10^{-6}$, indicating nearly perfect satisfaction of the physical smoothness requirement.

\subsection{Model Predictions on Test Energies}

Figure~\ref{fig:comparison} compares model predictions against simulation truth for the withheld test energies.

\begin{figure}[htb]
\centering
\includegraphics[width=0.9\textwidth]{model_comparison}
\caption{\textit{What students should observe:} Stopping power predictions for withheld test energies (200 and 750~MeV) with uncertainty quantification. \textbf{Shaded regions} show $\pm 1\sigma$ prediction uncertainty. Students observe that the physics-informed model achieves both lower bias (predictions closer to simulation truth) and lower uncertainty (narrower bands) than the baseline. This demonstrates that physics constraints improve not just accuracy, but confidence calibration.}
\label{fig:comparison}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.8\textwidth]{uncertainty_comparison}
\caption{\textit{What students should observe:} Prediction uncertainty comparison across the energy range. The physics-informed network exhibits systematically lower uncertainty, with largest improvements at intermediate energies where training data is sparse. This teaches students that honest uncertainty quantification---knowing what you don't know---is a hallmark of reliable predictions.}
\label{fig:uncertainty}
\end{figure}



\subsection{Computational Considerations}

\textit{What students learn:} Practical physics requires understanding computational trade-offs. Table~\ref{tab:cost} compares the computational requirements of each approach, helping students appreciate why surrogate models (like trained neural networks) are valuable for interactive exploration.

\begin{table}[htb]
\caption{\textit{Teaching point:} Computational cost comparison across validation methods. Students observe that Monte Carlo provides high fidelity but at high cost, analytical methods are fast but approximate, and trained neural networks offer a practical middle ground for rapid exploration.}
\centering
\begin{tabular}{l c c c c}
\toprule
\textbf{Method} & \textbf{Training} & \textbf{Inference (1 pt)} & \textbf{Inference (1000 pts)} & \textbf{Memory} \\
\midrule
TOPAS Monte Carlo & N/A & $\sim$12 hours & $\sim$120 hours & 2 GB \\
Bethe-Bloch Analytical & N/A & 0.001 ms & 1 ms & $<$1 MB \\
Baseline Neural Network & 15 min & 0.01 ms & 10 ms & 45 MB \\
Physics-Informed NN & 45 min & 0.01 ms & 10 ms & 45 MB \\
\bottomrule
\end{tabular}
\label{tab:cost}
\end{table}

Students learn that the trained neural network achieves simulation-quality predictions with analytical-level speed. This dramatic difference in inference time illustrates why surrogate models are valuable: they enable rapid exploration of parameter spaces that would be prohibitively expensive to simulate directly. The longer training time for the physics-informed variant is acceptable given its improved generalization to unseen conditions.

%% ============================================================================
%% SECTION 5: EDUCATIONAL IMPLEMENTATION
%% ============================================================================
\section{Educational Implementation}

\subsection{Learning Objectives and Student Outcomes}

This framework addresses core learning objectives for advanced undergraduate and early graduate physics students. After completing activities based on this framework, students should be able to:

\begin{enumerate}
    \item \textbf{Apply the validation mindset:} Explain why computational physics results must be verified against multiple independent methods, and identify at least three reasons why a single method is insufficient.
    
    \item \textbf{Select appropriate methods:} Given a physics problem, articulate when Monte Carlo simulation, analytical theory, or machine learning approaches are most appropriate, and identify their respective strengths and limitations.
    
    \item \textbf{Integrate physics knowledge with AI:} Demonstrate how embedding domain knowledge (physics constraints) into neural networks improves generalization, and explain why this matters for reliable predictions.
    
    \item \textbf{Interpret discrepancies critically:} When presented with disagreement between calculation methods, distinguish between computational errors, missing physics, and differences in what each method represents. Apply this to the TOPAS-NIST discrepancy as a worked example.
    
    \item \textbf{Practice reproducible science:} Document computational workflows, share code and data, and verify that results can be reproduced by others.
\end{enumerate}

\subsection{Target Audience and Prerequisites}

This framework is designed for:

\begin{itemize}
    \item \textbf{Advanced undergraduate students} (3rd--4th year) in physics, with prior exposure to computational methods and basic Python programming.
    \item \textbf{Early MSc students} beginning research projects involving simulation or machine learning.
    \item \textbf{Course contexts:} Computational physics, nuclear instrumentation laboratories, radiation physics, or introductory medical physics modules.
\end{itemize}

\textbf{Prerequisites:} Familiarity with Python, basic understanding of particle physics concepts (energy loss, stopping power), and exposure to either Monte Carlo methods or neural networks (not necessarily both).

\subsection{How This Differs from Traditional Teaching}

Traditional radiation transport courses teach methods in isolation---the Bethe-Bloch formula in one lecture, Monte Carlo concepts in another, perhaps machine learning in a separate course entirely. Students rarely see these methods in dialogue.

This framework differs by:

\begin{itemize}
    \item \textbf{Integrating methods from the start:} Students encounter analytical, simulation, and ML approaches applied to the same physical system, making their complementarity immediately apparent.
    
    \item \textbf{Emphasizing disagreement as learning:} Rather than treating discrepancies as problems to be minimized, the framework positions them as opportunities for physics insight.
    
    \item \textbf{Teaching constraint validation:} The ablation study demonstrates that even physics-motivated constraints require empirical testing; a lesson rarely explicit in traditional curricula.
    
    \item \textbf{Providing complete, runnable examples:} Rather than describing methods abstractly, students work with concrete implementations they can modify and extend.
\end{itemize}

\subsection{Classroom Activities}

We provide three structured activities suitable for computational physics courses:

\textbf{Activity 1: Monte Carlo exploration (2 hours).} Students analyze pre-computed TOPAS simulation outputs, identify Bragg peak features, and investigate how peak position correlates with incident energy. Extension: students modify simulation parameters and predict outcomes before running.

\textbf{Activity 2: Reference data comparison (1 hour).} Students extract mean stopping powers from simulation data, compare systematically against NIST tabulated values, compute ratios across energies, and develop physics-based explanations for observed discrepancies.

\textbf{Activity 3: Physics-informed ML investigation (2 hours).} Using pre-trained models, students evaluate predictions on withheld energies, quantitatively compare baseline versus physics-constrained performance, and conduct their own constraint ablation to discover which physics principles most benefit generalization.

\subsection{Connection to Professional Practice}

The validation methodology taught here parallels professional practice in multiple fields:

\begin{itemize}
    \item \textbf{Medical physics:} Treatment planning systems undergo commissioning where Monte Carlo simulations are validated against measurements and analytical benchmarks before clinical use \cite{AAPM2004}.
    
    \item \textbf{Nuclear engineering:} Reactor physics codes are validated against experimental benchmarks and cross-checked between independent simulation packages.
    
    \item \textbf{High-energy physics:} Detector simulations are validated against test beam data before being used for physics analysis.
\end{itemize}

Students learning multi-method validation develop skills transferable across these and other domains where computational reliability matters.

\textbf{Important scope limitation:} We validate scintillator detector response, not patient dose. Clinical applications would require tissue-equivalent materials, full 3D geometries, clinical validation datasets, and regulatory approval---years of additional work beyond this educational framework.

\subsection{Open Educational Resources}

All materials are available at: \texttt{https://github.com/Yash-Programmer/pinn-radiation-validation}

\begin{itemize}
    \item OpenTOPAS configuration files for all 14 energies
    \item Pre-processed training data (CSV format, 1400 samples)
    \item Pre-trained models (TensorFlow/Keras: baseline\_nn.keras, advanced\_pinn.keras)
    \item Instructor notes with learning objectives, assessment rubrics, and discussion prompts
    \item Solutions and expected results for instructor reference
\end{itemize}

%% ============================================================================
%% SECTION 6: DISCUSSION
%% ============================================================================
\section{Discussion}

\subsection{Why Physics Constraints Improve Generalization: A Pedagogical Perspective}

The substantial improvement from physics-informed constraints illustrates a fundamental principle that students often miss: machine learning works best when combined with domain knowledge, not when used as a pure black-box interpolator.

An unconstrained neural network can fit training data with any smooth functiondata, including predictions that discontinuously jump between adjacent energies, or that predict negative energy deposition. Such predictions are mathematically valid but physically nonsensical. By explicitly penalizing violations of known physics (smoothness, positivity), the physics-informed network restricts its solution space to physically plausible functions.

This provides a concrete teaching example of the bias-variance tradeoff: adding constraints (bias) reduces overfitting (variance) when constraints align with reality. Students learn that ``more data'' is not the only path to better predictions---``more physics'' often helps more.

\subsection{Why Disagreement Between Models is Educational}

When models agree, students gain confidence. When models disagree, students learn.

The discrepancy between TOPAS simulations and NIST reference data is not a failure---it is a teaching moment. NIST reports electronic stopping power only; TOPAS simulates full physics including nuclear interactions. The apparent 17\% ``disagreement'' at high energies actually reveals that these methods answer different physical questions. Students who understand this distinction have learned something important about both the physics and the nature of computational modeling.

This principle---treating disagreement as information rather than error---is transferable to any computational physics domain. Cosmological simulations disagree with observations in ways that reveal missing physics. Fluid dynamics codes disagree at turbulence scales in ways that illuminate modeling assumptions. Students who learn to interrogate disagreement become better scientists.

\subsection{Why Blind Trust in AI is Dangerous}

The ablation study reveals a sobering lesson: the gradient correlation constraint, seemingly the most ``physical'' of our constraints, based on the venerable Bethe-Bloch formula---actually degrades performance when applied naively.

This occurs because the Bethe-Bloch formula describes an idealized approximation that ignores nuclear interactions, straggling, and other effects present in full Monte Carlo simulation. Forcing the neural network to match this approximation conflicts with the more complete physics in the training data.

The pedagogical lesson is profound: even physics knowledge must be applied thoughtfully. Students who learn that ``adding physics'' requires validation---not just good intentions---will be more effective practitioners of physics-informed machine learning.

\subsection{Validation as a Transferable Physics Skill}

The validation mindset taught by this framework applies far beyond radiation transport:

\begin{itemize}
    \item \textbf{Computational fluid dynamics:} Simulations are validated against wind tunnel experiments and analytical solutions in limiting cases.
    \item \textbf{Quantum chemistry:} Density functional calculations are benchmarked against higher-level methods and experimental measurements.
    \item \textbf{Climate modeling:} Predictions are compared across independent modeling centers and against paleoclimate data.
    \item \textbf{Machine learning in science:} Any AI model applied to physical systems should be validated against physics-based methods.
\end{itemize}

Students who internalize multi-method validation as a habit of mind will be better prepared for research careers where computational tools are ubiquitous and reliability is essential.

\subsection{Comparison with Existing Educational Approaches}

To our knowledge, this represents the first educational framework applying physics-informed neural networks to radiation transport with explicit validation against multiple independent methods. Previous educational work on PINNs \cite{Raissi2019} focused on differential equations without specific physics domains or multi-method validation. Standard radiation physics courses teach Monte Carlo and analytical methods separately without systematic comparison.

Our contribution connects abstract PINN methodology to concrete particle physics applications, while positioning the entire exercise within a validation framework that teaches transferable skills.

\subsection{Limitations and Future Extensions}

\textbf{Current limitations:}
\begin{enumerate}
    \item \textbf{1D geometry:} Our depth-only approach is pedagogically accessible but less realistic than full 3D treatment.
    \item \textbf{Limited materials:} Extending to tissue-equivalent materials would strengthen connections to medical physics.
    \item \textbf{No formal assessment:} We have not yet developed validated instruments to measure student learning outcomes.
\end{enumerate}

\textbf{Future extensions:}
\begin{enumerate}
    \item Develop and validate assessment instruments measuring student mastery of validation skills.
    \item Extend the framework to other physics domains (electromagnetic showers, neutron transport) to demonstrate generalizability.
    \item Create adaptations for different course levels and contexts.
    \item Collect student feedback and learning outcomes data from classroom implementations.
\end{enumerate}

%% ============================================================================
%% SECTION 7: CONCLUSIONS
%% ============================================================================
\section{Conclusions}

\subsection{The Teaching Problem Revisited}

Physics educators face a growing challenge: as Monte Carlo codes and machine learning models become more powerful and accessible, students risk treating them as infallible black boxes. The critical skill of multi-method validation, defined as comparing results from fundamentally different calculation approaches, remains under-taught in standard curricula. This manuscript addresses that pedagogical gap.

\subsection{What Students Learn}

Through this framework, students develop competencies essential for modern computational physics:

\begin{enumerate}
    \item \textbf{The validation mindset:} Computational results gain credibility when multiple independent methods agree, and where disagreements reveal physics rather than error.
    
    \item \textbf{Critical evaluation of AI:} Machine learning predictions improve substantially when informed by physics constraints, but constraint selection itself requires validation. ``Adding physics'' is not automatic.
    
    \item \textbf{Interpreting discrepancies:} The systematic excess in TOPAS versus NIST stopping powers teaches that apparent disagreement often reflects complementary physics (nuclear interactions in this case), not computational failure.
    
    \item \textbf{Reproducible practice:} All materials are openly shared, teaching by example that computational physics should be reproducible and transparent.
\end{enumerate}

\subsection{Transferability Across Physics Domains}

The validation framework taught here transfers to any computational physics domain where multiple calculation modalities exist: fluid dynamics, quantum chemistry, climate modeling, detector simulation. Students who internalize multi-method validation as standard practice become more effective researchers and professionals, regardless of their specific field.

\subsection{Classroom Adoption}

All simulation configurations, pre-trained models, Jupyter notebooks, and instructor guides are provided as open educational resources at \texttt{https://github.com/Yash-Programmer/pinn-radiation-validation}. We invite physics educators to adapt this framework to their courses and welcome feedback on classroom implementations.

\subsection{Closing Reflection}

This framework teaches more than radiation transport physics---it teaches the habits of mind that distinguish computational scientists who produce reliable results from those who produce impressive-looking but unvalidated outputs. In an era of increasingly powerful computational tools, this distinction matters. We prepare future physicists to develop, validate, and responsibly deploy computational methods in domains where accuracy has consequences.

%% ============================================================================
%% ACKNOWLEDGMENTS AND DATA
%% ============================================================================

\ack{The authors thank the OpenTOPAS development community for simulation support and extensive documentation.}

\data{All simulation data, trained models, and analysis code are available at \texttt{https://github.com/Yash-Programmer/pinn-radiation-validation} under the MIT License. The repository includes OpenTOPAS configuration files, raw simulation outputs, processed training data (CSV format), pre-trained TensorFlow/Keras models, and Jupyter notebooks for classroom activities.}

\suppdata{Jupyter notebooks for classroom activities and instructor notes are available in the supplementary materials. Additional figures showing all 14 energy profiles are provided in the online supplement.}

%% ============================================================================
%% REFERENCES
%% ============================================================================
\section*{References}

\begin{thebibliography}{99}

\bibitem{AAPT2016}
AAPT Committee on Laboratories 2016 \textit{AAPT Recommendations for Computational Physics in the Undergraduate Physics Curriculum} (College Park, MD: American Association of Physics Teachers)

\bibitem{AAPM2004}
AAPM Task Group 65 2004 Tissue inhomogeneity corrections for megavoltage photon beams \textit{Med. Phys.} \textbf{31} 2083--115

\bibitem{Agostinelli2003}
Agostinelli S \textit{et al} (Geant4 Collaboration) 2003 Geant4---a simulation toolkit \textit{Nucl. Instrum. Methods Phys. Res. A} \textbf{506} 250--303

\bibitem{Bethe1930}
Bethe H 1930 Zur Theorie des Durchgangs schneller Korpuskularstrahlen durch Materie \textit{Ann. Phys.} \textbf{397} 325--400

\bibitem{Bloch1933}
Bloch F 1933 Zur Bremsung rasch bewegter Teilchen beim Durchgang durch Materie \textit{Ann. Phys.} \textbf{408} 285--320

\bibitem{ICRU2014}
ICRU 2014 \textit{ICRU Report 90: Key Data for Ionizing-Radiation Dosimetry: Measurement Standards and Applications} (Bethesda, MD: International Commission on Radiation Units and Measurements)

\bibitem{Karniadakis2021}
Karniadakis G E, Kevrekidis I G, Lu L, Perdikaris P, Wang S and Yang L 2021 Physics-informed machine learning \textit{Nat. Rev. Phys.} \textbf{3} 422--40

\bibitem{NIST2023}
Berger M J, Coursey J S, Zucker M A and Chang J 2023 \textit{ESTAR, PSTAR, and ASTAR: Computer Programs for Calculating Stopping-Power and Range Tables for Electrons, Protons, and Helium Ions} (Gaithersburg, MD: National Institute of Standards and Technology) available at \url{https://physics.nist.gov/Star}

\bibitem{Perl2012}
Perl J, Shin J, Schumann J, Faddegon B and Paganetti H 2012 TOPAS: An innovative proton Monte Carlo platform for research and clinical applications \textit{Med. Phys.} \textbf{39} 6818--37

\bibitem{Raissi2019}
Raissi M, Perdikaris P and Karniadakis G E 2019 Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations \textit{J. Comput. Phys.} \textbf{378} 686--707

\bibitem{Vandervoort2016}
Vandervoort E and Bhide S 2016 Machine learning and artificial intelligence in radiation therapy treatment planning \textit{Semin. Radiat. Oncol.} \textbf{26} 344--55

\bibitem{Gal2016}
Gal Y and Ghahramani Z 2016 Dropout as a Bayesian approximation: Representing model uncertainty in deep learning \textit{Proc. 33rd Int. Conf. Machine Learning} \textbf{48} 1050--59

\end{thebibliography}

\end{document}
